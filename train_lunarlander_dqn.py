# ==========================================
# ğŸš€ TRAINING A DQN AGENT ON LUNARLANDER-V2
# ==========================================

# ğŸ”§ Import the Gymnasium environment and Stable Baselines 3 DQN algorithm
import gymnasium as gym                      # For the game environment
from stable_baselines3 import DQN            # Deep Q-Network algorithm

# ------------------------------------------
# ğŸ§± STEP 1: Create the LunarLander Environment
# ------------------------------------------
# LunarLander-v2 is a 2D physics simulation where the goal is to land a lunar module softly.
# No rendering is needed here since we're just training (no visuals).
env = gym.make("LunarLander-v3")           # Environment handles simulation logic

# ------------------------------------------
# ğŸ¤– STEP 2: Initialize the DQN Agent
# ------------------------------------------
# MlpPolicy = Multi-Layer Perceptron policy (fully connected neural net)
# verbose=1 lets us see training progress in the terminal
model = DQN("MlpPolicy", env, verbose=1)

# ------------------------------------------
# ğŸ‹ï¸ STEP 3: Train the Agent
# ------------------------------------------
# The agent will interact with the environment for 200,000 timesteps.
# This is where most of the time is spent.
# ğŸ’¡ Estimated time:
#    - Mid-range CPU (no GPU): ~20â€“40 mins
#    - High-end CPU: ~10â€“20 mins
#    - GPU (RTX 3060+): ~3â€“10 mins
#    - For quick tests, try 10_000 steps instead of 200_000
model.learn(total_timesteps=200_000)

# ------------------------------------------
# ğŸ’¾ STEP 4: Save the Trained Model
# ------------------------------------------
# This saves two files:
#   - lunarlander_dqn.zip (model weights and config)
#   - Additional metadata for loading later
model.save("lunarlander_dqn")                # This creates lunarlander_dqn.zip

# ------------------------------------------
# âœ… DONE
# ------------------------------------------
print("âœ… Model trained and saved as lunarlander_dqn.zip")
